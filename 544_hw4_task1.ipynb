{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"name":"544_hw4_task1.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"2iyphN7FVdRj"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"2iyphN7FVdRj","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SCeSXQg1WA42"},"source":["import os\n","os.chdir(\"/content/drive/My Drive/Colab Notebooks\")\n","!ls"],"id":"SCeSXQg1WA42","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"36510eb4"},"source":["import pandas as pd\n","\n","import csv\n","import os\n","path = '/content/drive/My Drive/Colab Notebooks/'"],"id":"36510eb4","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5isg-2a5fJ8-"},"source":["### Depending on the input text format: <br>\n","#### there are 3 kinds:<br>\n","sentence_idx  &emsp;        word       &emsp;     tag<br>\n","1           &emsp;           EU         &emsp;     O<br>\n","2         &emsp;            word      &emsp;       O<br>\n","<br>\n","1            &emsp;       Blackburn   &emsp;   I-PER<br>\n","2           &emsp;         1996         &emsp;     O<br>\n","3          &emsp;           .          &emsp;      O<br>\n","<br>\n","1         &emsp;       -<DOCSTART>-     &emsp;     O<br>\n","2        &emsp;                       &emsp;       O<br>\n","\n","\n","#### Conclusion: <br>\n","If I want to move input file to torchtext dataset.SentencetTaggingDataset.split, I only need word and tag columns. And I also want to remove DOCSTART tag. <br>\n","We can also find that sentence split with blank line, not only in origina file, but also in input format for tortchtext."],"id":"5isg-2a5fJ8-"},{"cell_type":"code","metadata":{"id":"fD92kmy-rtQL"},"source":["def readfile(filename, csv_outfile, tsv_outfile):\n","  with open(filename, 'rb') as f:\n","    strip_line = (line.strip('\\n') for line in f)\n","    lines = (line.split(' ') for line in strip_line if line)\n","        \n","    with open(csv_outfile, 'w') as outfile:\n","        writer = csv.writer(outfile)\n","        writer.writerow(('sentence_idx', 'word', 'NER_tag'))\n","        writer.writerows(lines)\n","  \n","    data = pd.read_csv(filename)\n","    data = data.drop(df[df['word']=='-DOCSTART-'].index)\n","    data['word'].fillna('NA', inplace=True)\n","\n","    indices = data.loc[data['sentence_idx'] == 1].index.tolist()\n","    df = pd.DataFrame({'sentence_idx':0,'word':np.nan,'NER_tag':np.nan}, index=[i-0.5 for i in indices])\n","    df_new = data.append(df)\n","    df_new = df_new.sort_index()\n","    df_new = df_new.reset_index(drop=True)\n","    df_new.drop[df_new['sentence_idx'].index]\n","    df_new.drop('sentence_idx', axis=1)\n","    df_new.drop(-0.5,axis=0)\n","\n","  df_new.to_csv(tsv_outfile, sep = '\\t', index=False, header=False)\n","    \n","  return True"],"id":"fD92kmy-rtQL","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yuRU5fd8l51I"},"source":["## I have already done these steps in other file, I just use the cleaned data for training."],"id":"yuRU5fd8l51I"},{"cell_type":"code","metadata":{"id":"PLnk6Ml4W7cH"},"source":["! pip install torchtext==0.6.0"],"id":"PLnk6Ml4W7cH","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5145658e"},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","from torchtext.data import Field, BucketIterator\n","from torchtext.datasets import SequenceTaggingDataset\n","\n","import spacy\n","import numpy as np\n","\n","import time\n","import random"],"id":"5145658e","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ciLau2UbbPV_"},"source":["import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)"],"id":"ciLau2UbbPV_","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NfmQRWltXxSy"},"source":["import torch.nn.functional as F"],"id":"NfmQRWltXxSy","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"94dc470a"},"source":["SEED = 1000\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True"],"id":"94dc470a","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JFivwqeWtTZP"},"source":["class Data(object):\n","\n","  def __init__(self, path, batch_size):\n","    # list all the fields\n","    self.word_field = Field(lower=False)\n","    self.tag_field = Field(unk_token=None)\n","    # create dataset using built-in parser from torchtext\n","    self.train_dataset, self.val_dataset = SequenceTaggingDataset.splits(path=path, train=\"train.tsv\", validation=\"dev.tsv\", fields=((\"word\", self.word_field), (\"tag\", self.tag_field)))\n","    #self.test_dataset = SequenceTaggingDataset.splits(path=path, test=\"test1.tsv\", fields=((\"word\", self.word_field), (\"tag\", self.tag_field)))\n","    # convert fields to vocabulary list\n","    # min_freq is min word frequency, words occurring less than 3 times will be ignored from vocab\n","    self.word_field.build_vocab(self.train_dataset.word, min_freq=3)\n","    self.tag_field.build_vocab(self.train_dataset.tag)\n","    \n","    # create iterator for batch input\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    self.train_iter, self.val_iter = BucketIterator.splits(datasets=(self.train_dataset, self.val_dataset),batch_size=batch_size, device=device)\n","    #self.test_iter = BucketIterator.splits(datasets=(self.train_dataset, self.val_dataset, self.test_dataset),batch_size=batch_size, device=device)\n","    # prepare padding index to be ignored during model training/evaluation\n","    self.word_pad_idx = self.word_field.vocab.stoi[self.word_field.pad_token]\n","    self.tag_pad_idx = self.tag_field.vocab.stoi[self.tag_field.pad_token]"],"id":"JFivwqeWtTZP","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0a00555c"},"source":["data = Data(\n","    path=path,\n","    batch_size=16\n",")"],"id":"0a00555c","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U0BjpKdRK01u"},"source":["print('This train dataset have %d sentences.'%len(data.train_dataset))\n","print('This validation dataset have %d sentences.'%len(data.val_dataset))"],"id":"U0BjpKdRK01u","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XKCyPSrpCv-6"},"source":["print(data.tag_field.pad_token)"],"id":"XKCyPSrpCv-6","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GlOl-RjuC62w"},"source":["print(data.word_field.vocab.stoi)"],"id":"GlOl-RjuC62w","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BC-WqOjvEH-L"},"source":["print(data.tag_field.vocab.stoi)"],"id":"BC-WqOjvEH-L","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"71c1c97d"},"source":["class BiLSTM(nn.Module):\n","\n","    def __init__(self, embedding_dim, hidden_dim, input_dim, emb_dropout,\n","                 lstm_dropout, fc_dropout, output_dim, word_pad_idx):\n","        super(BiLSTM, self).__init__()\n","        self.embedding_dim = embedding_dim\n","        self.hidden_dim = hidden_dim\n","        self.input_dim = input_dim\n","        self.emb_dropout = emb_dropout\n","        self.fc_dropout = fc_dropout\n","        self.output_dim = output_dim\n","        self.word_pad_idx = word_pad_idx\n","        \n","        self.word_embeddings = nn.Embedding(num_embeddings=input_dim, embedding_dim=embedding_dim, padding_idx=word_pad_idx)\n","        # dropout before bilstm layer\n","        self.emb_dropout = nn.Dropout(emb_dropout)\n","        \n","        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim,\n","                            num_layers=1, bidirectional=True, dropout=lstm_dropout)\n","        # dropout after bilstm layer\n","        self.fc_dropout = nn.Dropout(fc_dropout)\n","        self.fc = nn.Linear(hidden_dim*2, output_dim)\n","        #self.fc_dropout = nn.Dropout(fc_dropout)\n","\n","    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n","        embedding_out = self.word_embeddings(sentence)\n","        lstm_out, _ = self.lstm(embedding_out)\n","        tag_space = self.fc(self.fc_dropout(lstm_out))\n","        tag_score = F.elu(tag_space)\n","        return tag_score\n","    \n","    def init_weight(self):\n","        for name, param in self.named_parameters():\n","            nn.init.normal_(param.data, mean=0, std=0.1)\n","            \n","    def count_parameters(self):\n","        return sum(p.numel() for p in self.parameters() if p.requires_grad)"],"id":"71c1c97d","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QeMwEh1rEDbs"},"source":["bilstm = BiLSTM(\n","    embedding_dim=100, hidden_dim=256, input_dim =len(data.word_field.vocab), emb_dropout=0.3, lstm_dropout=0.33, fc_dropout=0.25, output_dim=128, word_pad_idx = data.word_pad_idx)\n","print(f\"The model has {bilstm.count_parameters():,} trainable parameters.\")\n","print(bilstm)"],"id":"QeMwEh1rEDbs","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1d1_DcUUNPYT"},"source":["model = bilstm\n","optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","n_epochs = 10"],"id":"1d1_DcUUNPYT","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PbOELDN13Mt0"},"source":["from sklearn.metrics import confusion_matrix"],"id":"PbOELDN13Mt0","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TJaQFF461Emv"},"source":["class NER(object):\n","\n","  def __init__(self, model, data, optimizer, loss_fn):\n","    self.model = model\n","    self.data = data\n","    self.optimizer = optimizer\n","    self.loss_fn = loss_fn(ignore_index=self.data.tag_pad_idx)\n","\n","  @staticmethod\n","  def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs\n","\n","  def accuracy(self, preds, y):\n","    max_preds = preds.argmax(dim=1, keepdim=True)  # get the index of the max probability\n","    non_pad_elements = (y!=self.data.tag_pad_idx).nonzero()  # prepare masking for paddings\n","    correct = max_preds[non_pad_elements].squeeze(1)\n","    correct_sum = correct.eq(y[non_pad_elements]).sum()\n","    accuracy = correct_sum / torch.FloatTensor([y[non_pad_elements].shape[0]])\n","    \n","    return accuracy\n","\n","\n","  def epoch(self):\n","      epoch_loss = 0\n","      epoch_acc = 0\n","      self.model.train()\n","      for batch in self.data.train_iter:\n","        # text :sent len, batch size\n","        text = batch.word\n","        # tags :sent len, batch size\n","        true_tags = batch.tag\n","        self.optimizer.zero_grad()\n","        pred_tags = self.model(text)\n","        # to calculate the loss and accuracy, we flatten both prediction and true tags\n","        # flatten pred_tags to [sent len, batch size, output dim]\n","        pred_tags = pred_tags.view(-1, pred_tags.shape[-1])\n","        # flatten true_tags to [sent len * batch size]\n","        true_tags = true_tags.view(-1)\n","        #confusion_matrix = confusion_matrix(true_tags, pred_tags)\n","        batch_loss = self.loss_fn(pred_tags, true_tags)\n","        batch_acc = self.accuracy(pred_tags, true_tags)\n","        batch_loss.backward()\n","        self.optimizer.step()\n","        epoch_loss += batch_loss.item()\n","        epoch_acc += batch_acc.item()\n","      return epoch_loss / len(self.data.train_iter), epoch_acc / len(self.data.train_iter)\n","\n","  def evaluate(self, iterator):\n","      epoch_loss = 0\n","      epoch_acc = 0\n","      self.model.eval()\n","      with torch.no_grad():\n","          for batch in iterator:\n","              text = batch.word\n","              true_tags = batch.tag\n","              pred_tags = self.model(text)\n","              pred_tags = pred_tags.view(-1, pred_tags.shape[-1])\n","              true_tags = true_tags.view(-1)\n","              #confu_matrix = confusion_matrix(true_tags, pred_tags)\n","              batch_loss = self.loss_fn(pred_tags, true_tags)\n","              batch_acc = self.accuracy(pred_tags, true_tags)\n","              epoch_loss += batch_loss.item()\n","              epoch_acc += batch_acc.item()\n","      return epoch_loss / len(iterator), epoch_acc / len(iterator)\n","\n","  # main training sequence\n","  def train(self, n_epochs):\n","    for epoch in range(n_epochs):\n","        start_time = time.time()\n","        train_loss, train_acc = self.epoch()\n","        end_time = time.time()\n","        epoch_mins, epoch_secs = NER.epoch_time(start_time, end_time)\n","        print(f\"Epoch: {epoch + 1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\")\n","        print(f\"\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%\")\n","        val_loss, val_acc = self.evaluate(self.data.val_iter)\n","        print(f\"\\tValidation Loss: {val_loss:.3f} | Validation Acc: {val_acc * 100:.2f}%\")\n","    print(f\"\\tValidation Loss: {val_loss:.3f} | Validation Acc: {val_acc * 100:.2f}%\")\n","    #print(f\"\\tVallidation Confusion Metrix: {confu_matrix:.3f}%\")\n","\n","  def infer(self, sentence, true_tags=None, sentence_idxs=None):\n","    self.model.eval()\n","    # tokenize sentence\n","    tokens = [token for token in sentence]\n","    # transform to indices based on corpus vocab\n","    numericalized_tokens = [self.data.word_field.vocab.stoi[t] for t in tokens]\n","    # begin prediction\n","    token_tensor = torch.LongTensor(numericalized_tokens)\n","    token_tensor = token_tensor.unsqueeze(-1)\n","    predictions = self.model(token_tensor)\n","    # convert results to tags\n","    top_predictions = predictions.argmax(-1)\n","    predicted_tags = [self.data.tag_field.vocab.itos[t.item()] for t in top_predictions]\n","    # print inferred tags\n","    #max_len_token = len(tokens) + len(\"word\")\n","    #max_len_tag = len(predicted_tags) + len(\"pred\")\n","    #print('sentence_idx.       word.      gold tag.       pred tag\" )\n","    #pred_outfile = open('/content/drive/My Drive/Colab Notebooks/dev1.out', 'w')\n","    #for i, token in enumerate(tokens):\n","      #print(predicted_tags[i])\n","      #pred_outfile.write(str(sentence_idxs[i]) + '\\t' + token + '\\t' + true_tags[i] + '\\t' + predicted_tags[i] + \"\\n\")\n","    #pred_outfile.close()\n","    return predicted_tags"],"id":"TJaQFF461Emv","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dKS0swe78uMi"},"source":["ner = NER(\n","  model=bilstm,\n","  data=data,\n","  optimizer=optimizer,\n","  loss_fn=nn.CrossEntropyLoss\n",")"],"id":"dKS0swe78uMi","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H__5rDs0TrYV"},"source":["ner.train(n_epochs)"],"id":"H__5rDs0TrYV","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xHTPMcdg2RL4"},"source":["# save model\n","torch.save(model, '/content/drive/My Drive/Colab Notebooks/blstm1.pt')"],"id":"xHTPMcdg2RL4","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NbMfASF2SX19"},"source":["#dev\n","dev_df = pd.read_csv('dev.csv')\n","dev_df.head()"],"id":"NbMfASF2SX19","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fs_tZI9aHJ7z"},"source":["# get prediction for dev file\n","dev_sentence = dev_df['word'].tolist()\n","dev_sentence_idx = dev_df['sentence_idx'].tolist()\n","dev_true_tags = dev_df['NER_tag'].tolist"],"id":"Fs_tZI9aHJ7z","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eQBpgV40TCbk"},"source":["dev_pred_tags = ner.infer(sentence=dev_sentence, true_tags=dev_true_tags, sentence_idxs=dev_sentence_idx)"],"id":"eQBpgV40TCbk","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5A2siDFZeZZT"},"source":["dev_pred_tags_df = pd.DataFrame({'pred_tag': dev_pred_tags})\n","new_dev_df = pd.concat([dev_df,dev_pred_tags_df], axis=1)\n","new_dev_df"],"id":"5A2siDFZeZZT","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QI2PjKWUgucS"},"source":["new_dev_df.to_csv('/content/drive/My Drive/Colab Notebooks/dev1_eval.txt', sep = ' ', index=False, header=False)"],"id":"QI2PjKWUgucS","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BbGnyeXNvfyF"},"source":["out_dev_df = new_dev_df.drop(columns='NER_tag')\n","out_dev_df.head()"],"id":"BbGnyeXNvfyF","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OEIQwYJfvvg-"},"source":["out_dev_df.to_csv('/content/drive/My Drive/Colab Notebooks/dev1.out', sep = ' ', index=False, header=False)"],"id":"OEIQwYJfvvg-","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g1A_BCCfNhJx"},"source":["# get prediction for text file\n","test_df = pd.read_csv('test.csv')\n","test_df.head()\n"],"id":"g1A_BCCfNhJx","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WauaGrKsuWuY"},"source":["test_sentence = test_df['word'].tolist()\n","test_sentence_idx = test_df['sentence_idx'].tolist()"],"id":"WauaGrKsuWuY","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jjQ1VNS2uhRd"},"source":["test_pred_tags = ner.infer(sentence=test_sentence, sentence_idxs=test_sentence_idx)"],"id":"jjQ1VNS2uhRd","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M10U120BuudO"},"source":["test_pred_tags_df = pd.DataFrame({'pred_tag': test_pred_tags})\n","new_test_df = pd.concat([test_df,test_pred_tags_df], axis=1)\n","new_test_df.head()"],"id":"M10U120BuudO","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AQhhTRpcvBEU"},"source":["new_test_df.to_csv('/content/drive/My Drive/Colab Notebooks/test1.out', sep = ' ', index=False, header=False)"],"id":"AQhhTRpcvBEU","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Casu6RIth-fD"},"source":["!pip install perl"],"id":"Casu6RIth-fD","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8lmxft9OwIB1"},"source":[""],"id":"8lmxft9OwIB1","execution_count":null,"outputs":[]}]}